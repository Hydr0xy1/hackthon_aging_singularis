#!/usr/bin/env python3
"""
è¯­ä¹‰æ„ŸçŸ¥çš„IMRaDçŸ¥è¯†å›¾è°±æå–ç³»ç»Ÿ
è§£å†³åŒä¸€å•è¯åœ¨ä¸åŒè¯­å¢ƒä¸­çš„è¯­ä¹‰æ­§ä¹‰é—®é¢˜
"""

import sys
import os
import time
import json
import csv
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict, Counter

# å¯¼å…¥è¯­ä¹‰æå–å™¨
try:
    from src.semantic_extractor import SemanticIMRaDExtractor
    SEMANTIC_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  è¯­ä¹‰æå–å™¨å¯¼å…¥å¤±è´¥: {e}")
    SEMANTIC_AVAILABLE = False

# å¯¼å…¥å…¶ä»–å¿…è¦æ¨¡å—
try:
    import fitz
    import re
    import uuid
    from collections import Counter, defaultdict
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

class SemanticIMRaDPipeline:
    """è¯­ä¹‰æ„ŸçŸ¥çš„IMRaDå¤„ç†ç®¡é“"""
    
    def __init__(self):
        self.semantic_extractor = SemanticIMRaDExtractor() if SEMANTIC_AVAILABLE else None
        
        # IMRaDç« èŠ‚æ¨¡å¼
        self.section_patterns = {
            "introduction": r"(?:^|\n)(?:\s*\d*\.*\s*Introduction|Background)",
            "methods": r"(?:^|\n)(?:\s*\d*\.*\s*(Materials and Methods|Methods|Experimental Procedures))",
            "results": r"(?:^|\n)(?:\s*\d*\.*\s*Results?)",
            "discussion": r"(?:^|\n)(?:\s*\d*\.*\s*(Discussion|Conclusion|Summary))",
        }
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """ä»PDFæå–æ–‡æœ¬"""
        print(f"ğŸ“„ ä» {os.path.basename(pdf_path)} æå–æ–‡æœ¬...")
        try:
            doc = fitz.open(pdf_path)
            text = ""
            for page_num, page in enumerate(doc):
                page_text = page.get_text("text")
                # æ¸…ç†æ–‡æœ¬
                lines = []
                for line in page_text.split("\n"):
                    line = line.strip()
                    if (not re.match(r"^\s*\d+\s*$", line) and 
                        len(line) > 15 and
                        not line.isupper() and
                        not re.match(r'^\s*[A-Z\s]+\s*$', line)):
                        lines.append(line)
                text += " ".join(lines) + "\n\n"
            doc.close()
            return text
        except Exception as e:
            print(f"âŒ PDFæå–é”™è¯¯: {e}")
            return ""
    
    def segment_imrad(self, text: str) -> Dict[str, str]:
        """IMRaDåˆ†æ®µ"""
        indices = []
        for name, pattern in self.section_patterns.items():
            matches = list(re.finditer(pattern, text, re.IGNORECASE | re.MULTILINE))
            if matches:
                indices.append((matches[0].start(), name))
        
        if not indices:
            print("âš ï¸  æœªæ£€æµ‹åˆ°æ ‡å‡†IMRaDç« èŠ‚ï¼Œä½¿ç”¨å…¨æ–‡å¤„ç†")
            return {"full_text": text}

        indices.sort()
        sections = {}
        
        for i, (start, name) in enumerate(indices):
            end = indices[i + 1][0] if i + 1 < len(indices) else len(text)
            section_content = text[start:end].strip()
            
            # ç§»é™¤ç« èŠ‚æ ‡é¢˜è¡Œ
            lines = section_content.split('\n')
            if len(lines) > 1:
                section_content = '\n'.join(lines[1:]).strip()
            
            sections[name] = section_content
        
        return sections
    
    def extract_nodes_semantic(self, text: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨è¯­ä¹‰ç†è§£æå–èŠ‚ç‚¹"""
        if not self.semantic_extractor:
            print("âš ï¸  è¯­ä¹‰æå–å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            return self._extract_nodes_traditional(text)
        
        print("ğŸ§  ä½¿ç”¨è¯­ä¹‰ç†è§£æå–èŠ‚ç‚¹...")
        sections = self.segment_imrad(text)
        nodes = self.semantic_extractor.extract_nodes_with_semantics(text, sections)
        
        # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
        if nodes:
            node_types = Counter([node["type"] for node in nodes])
            print(f"ğŸ“Š è¯­ä¹‰èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(node_types)}")
            
            # æ˜¾ç¤ºè¯­ä¹‰æ¶ˆæ­§ç¤ºä¾‹
            disambiguation_examples = [n for n in nodes if n.get("semantic_context", {}).get("disambiguation_applied")]
            if disambiguation_examples:
                print(f"ğŸ” åº”ç”¨äº†è¯­ä¹‰æ¶ˆæ­§çš„èŠ‚ç‚¹: {len(disambiguation_examples)} ä¸ª")
                for example in disambiguation_examples[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç¤ºä¾‹
                    print(f"   - {example['type']}: {example['text'][:60]}...")
        
        return nodes
    
    def _extract_nodes_traditional(self, text: str) -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿæ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰"""
        print("ğŸ“ ä½¿ç”¨ä¼ ç»Ÿæ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•...")
        
        # ä¼ ç»Ÿæ¨¡å¼
        cue_patterns = {
            "Hypothesis": [
                r"\bwe hypothesi[sz]e\b", r"\bwe propose\b", r"\bthis study aims to\b",
                r"\bwe expect\b", r"\bwe predict\b", r"\bit is hypothesized\b"
            ],
            "Experiment": [
                r"\bwe conducted\b", r"\bwe performed\b", r"\bexperiments\b",
                r"\bwe treated\b", r"\bmethods\b", r"\bexperimental\b"
            ],
            "Dataset": [
                r"\bcohort\b", r"\bn\s*=\s*\d+", r"\bdata from\b",
                r"\bpatients\b", r"\bsamples\b", r"\bdataset\b"
            ],
            "Analysis": [
                r"\bwe analyzed\b", r"\bstatistical analysis\b", r"\bp\s*[<â‰¤]\s*0\.\d+",
                r"\bsignificant\b", r"\bwe calculated\b", r"\bcorrelation\b"
            ],
            "Conclusion": [
                r"\bin conclusion\b", r"\bwe conclude\b", r"\bthese results suggest\b",
                r"\bthis study shows\b", r"\bour findings\b", r"\bthese data indicate\b"
            ]
        }
        
        sections = self.segment_imrad(text)
        nodes = []
        
        for section_name, section_text in sections.items():
            if len(section_text.strip()) < 100:
                continue
            
            # ç®€å•å¥å­åˆ†å‰²
            sentences = re.split(r'(?<=[.!?])\s+', section_text)
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 20:
                    continue
                
                matched = False
                for node_type, patterns in cue_patterns.items():
                    for pattern in patterns:
                        if re.search(pattern, sentence, re.IGNORECASE):
                            nodes.append({
                                "id": f"{node_type[:3].upper()}_{uuid.uuid4().hex[:6]}",
                                "type": node_type,
                                "text": sentence[:350],
                                "section": section_name,
                                "confidence": 0.8,
                                "evidence": f"pattern:{pattern}",
                                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                            })
                            matched = True
                            break
                    if matched:
                        break
        
        return nodes
    
    def build_semantic_edges(self, nodes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ„å»ºè¯­ä¹‰è¾¹å…³ç³»"""
        if not self.semantic_extractor:
            return self._build_traditional_edges(nodes)
        
        print("ğŸ”— æ„å»ºè¯­ä¹‰è¾¹å…³ç³»...")
        edges = self.semantic_extractor.build_semantic_edges(nodes)
        
        # ç»Ÿè®¡è¾¹ç±»å‹
        if edges:
            edge_types = Counter([edge["type"] for edge in edges])
            print(f"ğŸ“Š è¯­ä¹‰è¾¹ç±»å‹åˆ†å¸ƒ: {dict(edge_types)}")
        
        return edges
    
    def _build_traditional_edges(self, nodes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿè¾¹æ„å»ºæ–¹æ³•"""
        edges = []
        node_ids_by_type = defaultdict(list)
        
        for node in nodes:
            node_ids_by_type[node['type']].append(node['id'])
        
        edge_rules = [
            ("Hypothesis", "Experiment", "hypothesis_to_experiment"),
            ("Experiment", "Dataset", "experiment_to_dataset"), 
            ("Dataset", "Analysis", "dataset_to_analysis"),
            ("Analysis", "Conclusion", "analysis_to_conclusion"),
        ]
        
        for src_type, tgt_type, rel_type in edge_rules:
            if src_type in node_ids_by_type and tgt_type in node_ids_by_type:
                count = 0
                for src_id in node_ids_by_type[src_type]:
                    for tgt_id in node_ids_by_type[tgt_type]:
                        edges.append({
                            "start": src_id,
                            "end": tgt_id,
                            "type": rel_type,
                            "confidence": 0.7
                        })
                        count += 1
                print(f"  ğŸ”— {rel_type}: {count} æ¡è¾¹")
        
        return edges
    
    def export_to_csv(self, nodes: List[Dict[str, Any]], edges: List[Dict[str, Any]], base_name: str):
        """å¯¼å‡ºä¸ºCSVæ–‡ä»¶"""
        Path("outputs").mkdir(exist_ok=True)
        
        # èŠ‚ç‚¹CSV
        if nodes:
            nodes_file = f"{base_name}_semantic_nodes.csv"
            with open(nodes_file, 'w', newline='', encoding='utf-8') as f:
                fieldnames = ['id', 'type', 'text', 'section', 'confidence', 'evidence', 'semantic_context', 'timestamp']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for node in nodes:
                    # å¤„ç†semantic_contextå­—æ®µ
                    row = {field: node.get(field, '') for field in fieldnames}
                    if 'semantic_context' in row and isinstance(row['semantic_context'], dict):
                        row['semantic_context'] = json.dumps(row['semantic_context'])
                    writer.writerow(row)
            print(f"  ğŸ’¾ è¯­ä¹‰èŠ‚ç‚¹ä¿å­˜: {nodes_file}")
        
        # è¾¹CSV  
        if edges:
            edges_file = f"{base_name}_semantic_edges.csv"
            with open(edges_file, 'w', newline='', encoding='utf-8') as f:
                fieldnames = ['start', 'end', 'type', 'confidence', 'semantic_evidence']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for edge in edges:
                    writer.writerow(edge)
            print(f"  ğŸ’¾ è¯­ä¹‰è¾¹ä¿å­˜: {edges_file}")
    
    def create_semantic_visualization(self, nodes: List[Dict[str, Any]], edges: List[Dict[str, Any]], output_file: str):
        """åˆ›å»ºè¯­ä¹‰æ„ŸçŸ¥çš„å¯è§†åŒ–"""
        html_content = """
<!DOCTYPE html>
<html>
<head>
    <title>è¯­ä¹‰æ„ŸçŸ¥çš„IMRaDçŸ¥è¯†å›¾è°±</title>
    <meta charset="utf-8">
    <style>
        body { 
            font-family: 'Segoe UI', Arial, sans-serif; 
            margin: 20px; 
            background: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 { 
            color: #2c3e50; 
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .stats {
            background: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
        }
        .node { 
            margin: 10px 0; 
            padding: 15px; 
            border-radius: 8px; 
            color: white; 
            border-left: 5px solid rgba(0,0,0,0.2);
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .hypothesis { background: #e67e22; }
        .experiment { background: #3498db; }
        .dataset { background: #1abc9c; }
        .analysis { background: #9b59b6; }
        .conclusion { background: #e74c3c; }
        .semantic-info {
            background: #f8f9fa;
            padding: 8px;
            margin: 5px 0;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .disambiguation {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 8px;
            margin: 5px 0;
            border-radius: 4px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ§  è¯­ä¹‰æ„ŸçŸ¥çš„IMRaDçŸ¥è¯†å›¾è°±</h1>
        
        <div class="stats">
            <strong>è¯­ä¹‰æå–ç»Ÿè®¡:</strong><br>
            èŠ‚ç‚¹æ€»æ•°: %NODE_COUNT% | è¾¹å…³ç³»æ€»æ•°: %EDGE_COUNT%<br>
            è¯­ä¹‰æ¶ˆæ­§åº”ç”¨: %DISAMBIGUATION_COUNT% | å¤„ç†æ—¶é—´: %TIMESTAMP%
        </div>
        
        <h2>ğŸ”— è¯­ä¹‰è¾¹å…³ç³»</h2>
        %EDGES%
        
        <h2>ğŸ“ è¯­ä¹‰èŠ‚ç‚¹</h2>
        %NODES%
    </div>
</body>
</html>
        """
        
        # æŒ‰ç« èŠ‚åˆ†ç»„èŠ‚ç‚¹
        nodes_by_section = defaultdict(list)
        for node in nodes:
            nodes_by_section[node['section']].append(node)
        
        # ç”ŸæˆèŠ‚ç‚¹HTML
        nodes_html = ""
        disambiguation_count = 0
        
        for section_name, section_nodes in nodes_by_section.items():
            nodes_html += f'<div class="section" style="background: #2c3e50; color: white; padding: 10px 15px; margin: 20px 0 10px 0; border-radius: 5px; font-weight: bold;">ğŸ“ {section_name.upper()} ç« èŠ‚</div>'
            
            for node in section_nodes:
                # æ£€æŸ¥æ˜¯å¦åº”ç”¨äº†è¯­ä¹‰æ¶ˆæ­§
                semantic_context = node.get('semantic_context', {})
                disambiguation_applied = semantic_context.get('disambiguation_applied', False)
                if disambiguation_applied:
                    disambiguation_count += 1
                
                nodes_html += f"""
                <div class="node {node['type'].lower()}">
                    <div class="node-type" style="font-weight: bold; font-size: 1.1em;">{node['type']}</div>
                    <div class="node-text" style="margin: 8px 0; line-height: 1.4;">{node['text']}</div>
                    <div class="semantic-info">
                        <strong>è¯­ä¹‰è§’è‰²:</strong> {semantic_context.get('role', 'N/A')}<br>
                        <strong>å…³é”®å®ä½“:</strong> {', '.join(semantic_context.get('entities', [])[:5])}<br>
                        <strong>ç½®ä¿¡åº¦:</strong> {node.get('confidence', 'N/A')}
                    </div>
                    {f'<div class="disambiguation">ğŸ” å·²åº”ç”¨è¯­ä¹‰æ¶ˆæ­§</div>' if disambiguation_applied else ''}
                    <div class="node-meta" style="font-size: 0.9em; opacity: 0.8;">
                        ID: {node['id']} | è¯æ®: {node.get('evidence', 'N/A')}
                    </div>
                </div>
                """
        
        # ç”Ÿæˆè¾¹HTML
        edges_html = ""
        for edge in edges:
            semantic_evidence = edge.get('semantic_evidence', 'N/A')
            edges_html += f"""
            <div class="edge" style="margin: 8px 0; padding: 10px; background: #f8f9fa; border-left: 4px solid #34495e; border-radius: 4px;">
                <strong>{edge['start']}</strong> â†’ 
                <strong>{edge['end']}</strong> 
                <small>({edge['type']} | ç½®ä¿¡åº¦: {edge.get('confidence', 'N/A')})</small>
                <br><small>è¯­ä¹‰è¯æ®: {semantic_evidence}</small>
            </div>
            """
        
        # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
        html_content = html_content.replace("%NODE_COUNT%", str(len(nodes))) \
                                  .replace("%EDGE_COUNT%", str(len(edges))) \
                                  .replace("%DISAMBIGUATION_COUNT%", str(disambiguation_count)) \
                                  .replace("%TIMESTAMP%", time.strftime("%Y-%m-%d %H:%M:%S")) \
                                  .replace("%NODES%", nodes_html) \
                                  .replace("%EDGES%", edges_html)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"  ğŸŒ è¯­ä¹‰å¯è§†åŒ–: {output_file}")

def main(pdf_path: str):
    """è¿è¡Œè¯­ä¹‰æ„ŸçŸ¥çš„å®Œæ•´pipeline"""
    print(f"ğŸš€ å¼€å§‹è¯­ä¹‰æ„ŸçŸ¥å¤„ç†: {pdf_path}")
    start_time = time.time()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path("outputs").mkdir(exist_ok=True)
    
    # åˆ›å»ºå¤„ç†ç®¡é“
    pipeline = SemanticIMRaDPipeline()
    
    try:
        # æ­¥éª¤1: æå–æ–‡æœ¬
        text = pipeline.extract_text_from_pdf(pdf_path)
        if not text or len(text) < 100:
            print("âŒ æ–‡æœ¬æå–å¤±è´¥æˆ–æ–‡æœ¬è¿‡çŸ­")
            return
        
        print(f"âœ… æˆåŠŸæå–æ–‡æœ¬ ({len(text)} å­—ç¬¦)")
        
        # æ­¥éª¤2: è¯­ä¹‰èŠ‚ç‚¹æå–
        print("\nğŸ§  æ­£åœ¨è¿›è¡Œè¯­ä¹‰èŠ‚ç‚¹æå–...")
        nodes = pipeline.extract_nodes_semantic(text)
        
        if not nodes:
            print("âŒ æœªæå–åˆ°ä»»ä½•èŠ‚ç‚¹")
            return
        
        print(f"âœ… æˆåŠŸæå– {len(nodes)} ä¸ªè¯­ä¹‰èŠ‚ç‚¹")
        
        # æ­¥éª¤3: æ„å»ºè¯­ä¹‰è¾¹
        print("\nğŸ”— æ­£åœ¨æ„å»ºè¯­ä¹‰è¾¹å…³ç³»...")
        edges = pipeline.build_semantic_edges(nodes)
        print(f"âœ… æˆåŠŸæ„å»º {len(edges)} æ¡è¯­ä¹‰è¾¹")
        
        # æ­¥éª¤4: ä¿å­˜ç»“æœ
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜è¯­ä¹‰ç»“æœ...")
        base_name = f"outputs/{Path(pdf_path).stem}_semantic"
        
        pipeline.export_to_csv(nodes, edges, base_name)
        pipeline.create_semantic_visualization(nodes, edges, f"{base_name}_graph.html")
        
        elapsed_time = time.time() - start_time
        print(f"\nğŸ‰ è¯­ä¹‰å¤„ç†å®Œæˆ! ç”¨æ—¶ {elapsed_time:.2f} ç§’")
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(nodes)} ä¸ªè¯­ä¹‰èŠ‚ç‚¹, {len(edges)} æ¡è¯­ä¹‰è¾¹")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   - {base_name}_semantic_nodes.csv")
        print(f"   - {base_name}_semantic_edges.csv") 
        print(f"   - {base_name}_graph.html")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("âŒ ä½¿ç”¨æ–¹æ³•: python run_semantic_pipeline.py <PDFæ–‡ä»¶è·¯å¾„>")
        print("ğŸ’¡ ç¤ºä¾‹:")
        print('   python run_semantic_pipeline.py "data/artemisinin_pcos.pdf"')
        sys.exit(1)
    
    pdf_file = sys.argv[1]
    if not os.path.exists(pdf_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {pdf_file}")
        sys.exit(1)
    
    main(pdf_file)
